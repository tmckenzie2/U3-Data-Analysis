{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 310](https://github.com/GonzagaCPSC310) Data Mining\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Data Preprocessing\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn about the steps involved in data preprocessing\n",
    "* Learn about different attribute types\n",
    "* Summarize data with simple statistics\n",
    "* Clean data by filling missing values\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/28 Warm-up Task(s)\n",
    "* If you haven't done so already, go to our Github site, go to U2 Git and Github, go to B Github Classroom notes, and accept the hello world Github Classroom assignment under \"Practice Problem\"\n",
    "* From the command line, navigate to your GithubFun directory from last class\n",
    "* Copy your repository url and use it to run:\n",
    "    * `git remote add origin <your url>`\n",
    "    * `git push -u origin master`\n",
    "* Refresh your Github repository page in the browser and you should see your hello_world.py file   \n",
    "\n",
    "## 1/30 Warm-up Task(s)\n",
    "* Open DataPreprocessingFun/main.py\n",
    "* Given a table and a column index, write a function to get the column in the table at the index\n",
    "    * Ignore \"NA\" values\n",
    "* Given a table, write a function to get the min and max value of one of its (numerical) attributes\n",
    "* Test your functions on the following table:\n",
    "\n",
    "```\n",
    "header = [\"CarName\", \"ModelYear\", \"MSRP\"]\n",
    "msrp_table = [[\"ford pinto\", 75, 2769],\n",
    "              [\"toyota corolla\", 75, 2711],\n",
    "              [\"ford pinto\", 76, 3025],\n",
    "              [\"toyota corolla\", 77, 2789]]\n",
    "```\n",
    "\n",
    "* Brainstorm with your neighbor possible issues with the different approaches for dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data analysts spend a surprising amount of time preparing data for analysis. In fact, a survey was conducted found that cleaning big data is the most time-consuming and least enjoyable task data scientists do!\n",
    "<img src=\"http://blogs-images.forbes.com/gilpress/files/2016/03/Time-1200x511.jpg\" width=\"700\">\n",
    "(image from [http://blogs-images.forbes.com/gilpress/files/2016/03/Time-1200x511.jpg](http://blogs-images.forbes.com/gilpress/files/2016/03/Time-1200x511.jpg))\n",
    "\n",
    "The goal of data preprocessing is to produce high-quality data to improve mining results and efficiency\n",
    "\n",
    "At a high level, data preprocessing includes the following steps (these steps are done in any order and often multiple times):\n",
    "1. Data Exploration (basic understanding of meaning, attributes, values, issues)\n",
    "2. Data Reduction (reduce size via aggregation, redundant features, etc.)\n",
    "3. Data Integration (merge/combine multiple datasets)\n",
    "4. Data Cleaning (remove noise and inconsistencies)\n",
    "5. Data Transformation (normalize/scale, etc.)\n",
    "\n",
    "It is important for data mining that your process is transparent and repeatable:\n",
    "* Can repeat \"experiment\" and get the same result\n",
    "* No \"magic\" steps\n",
    "\n",
    "It is important, however, to write down steps (log):\n",
    "* Ideally, someone should be able to take your data, program, and description of steps, rerun everything, and get the same results!\n",
    "\n",
    "## Data Exploration\n",
    "Get to know your data first by exploring it. \n",
    "\n",
    "### Attributes\n",
    "Different aspects of attributes (variables)\n",
    "* Data (storage) type - e.g., int versus float versus string\n",
    "* Measurement scales - are values discrete or continuous\n",
    "* Semantic type – what the values represent (e.g., colors, ages)\n",
    "\n",
    "### Measurement Scales\n",
    "1. Nominal\n",
    "    * Discrete values without inherent order\n",
    "    * E.g., colors (red, blue, green), identifiers, occupation, gender\n",
    "    * Often ints or strings (but could be any data type)\n",
    "2. Ordinal\n",
    "    * Discrete values with inherent order\n",
    "    * E.g., t-shirt size (s, m, l, xl), grades (A+, A-, B+, ...)\n",
    "    * No guarantee that the difference between values is same\n",
    "    * Often ints or strings (but could be any data type)\n",
    "3. Interval\n",
    "    * Values measured on a scale of equal-sized widths\n",
    "    * Unlike ordinal, can compare and quantify difference between values\n",
    "    * No inherent zero point (i.e., absence)\n",
    "    * Temperature (Celsius, Fahrenheit) is an example\n",
    "4. Ratio\n",
    "    * Interval values with an inherent zero point\n",
    "    * Temperature in Kelvin is an example\n",
    "    * Also counts of things (where 0 means not present)\n",
    "    \n",
    "### Categorical vs Continuous\n",
    "* Categorical roughly means the nominal and ordinal values\n",
    "* Continuous roughly means the rest (interval, ratio) ... aka \"numerical\"\n",
    "* For many algorithms/approaches, this is enough detail\n",
    "\n",
    "### Labeled vs Unlabeled Data\n",
    "* Labeled data implies an attribute that classifies instances (e.g., mpg)\n",
    "    * Goal is typically to predict the class for new instances\n",
    "    * This is called \"Supervised Learning\"\n",
    "* Unlabeled means there isn't such an attribute (for mining purposes)\n",
    "    * Can still find patterns, associations, etc.\n",
    "    * Generally referred to as \"Unsupervised Learning\"\n",
    "            \n",
    "## Data Cleaning\n",
    "1. Noisy vs Invalid Values\n",
    "    * Noisy implies the value is correct, just recorded incorrectly\n",
    "        * E.g., decimal place error (5.72 instead of 57.2), wrong categorical value used\n",
    "    * Invalid implies a noisy value that is not a valid value (for domain)\n",
    "        * E.g., 57.2X, misspelled categorical data, or value out of range (6 on a 5 point scale)\n",
    "    * Ways to deal with this:\n",
    "        * Look for duplicates (when there shouldn't be)\n",
    "        * Look for outliers\n",
    "        * Sort and print range of values\n",
    "    * The term \"noisy\" may also imply random error or random variance\n",
    "        * Various techniques to \"smooth out\" values\n",
    "        * E.g., using means of bins or regression\n",
    "2. Missing Values\n",
    "     * How should we deal with missing values?\n",
    "        * Discard instances: throw out any row with a missing value\n",
    "        * Replace with a new value:\n",
    "            * By hand\n",
    "            * Use a constant\n",
    "            * Use a central tendency measure (mean, median, most frequent, ...)\n",
    "        * Most \"probable\" value (e.g., regression, using a classifier)\n",
    "        * Replace either across data set, or based on similar instances\n",
    "            * E.g. average based on model year\n",
    "        \n",
    "## Summary Statistics\n",
    "Summary statistics give (initial) insights into a dataset, such as:\n",
    "1. Number of instances (how many rows)\n",
    "2. Min and max attribute values\n",
    "    * Q: Do these make sense for both categorical and continuous attributes?\n",
    "        * Ordinal, but not Nominal\n",
    "        * Much easier if numeric!\n",
    "        * Can only count number of each nominal value\n",
    "    * Q: What should be done with null (NA) values?\n",
    "        * Really, undefined / unknown\n",
    "        * In practice just ignore them\n",
    "3. Middle values of a distribution (aka “Central Tendency”)\n",
    "    * Mid value: `(max + min) / 2.0` \n",
    "        * AKA \"Midrange\"\n",
    "    * (Arithmetic) Mean $\\bar{x} = (x_1 + x_2 + ... + x_n) / n$\n",
    "        * AKA average\n",
    "        * Python: `sum(column) / float(len(column))`\n",
    "        * Q: Problems with the mean? ... sensitive to extremes (e.g., outliers)\n",
    "        * Q: Make sense for categorical and continuous?\n",
    "            * only Interval or Ratio (same widths)\n",
    "    * Median\n",
    "        * The middle value in a set of sorted values\n",
    "        * If even number of values, halfway between the two middles\n",
    "        * Better measure for skewed data\n",
    "        * Can be expensive for large data sets (sorting!)\n",
    "    * Mode\n",
    "        * Value(s) that occurs most frequently\n",
    "    * typically assume data is unimodal (one mode), e.g., normally distributed\n",
    "    * Q: How might we compute the mode in Python?\n",
    "4. Data Dispersion (Spread)\n",
    "    * Range (max - min)\n",
    "    * Quantiles: (Roughly) equal size partitions of data (if sorted from smallest to largest)\n",
    "        * \"2-quantiles\" is the data point that divides into two halves (AKA median)\n",
    "        * \"Quartiles\" is three data points that divide into four groups\n",
    "            * Used as part of box plots (more later)\n",
    "        * Interquartile range (IQR) is distance between 1st and 3rd quartiles\n",
    "        * \"Percentiles\" are 100-quantiles (100 groups)\n",
    "    * Variance and Standard Deviation\n",
    "        * Variance measures how spread out the data is (small implies data close to mean, large implies data spread out) $$\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n}$$\n",
    "        * Standard Deviation is square root of variance\n",
    "            * Python: `numpy.std(vals)` \n",
    "                * ... more on numpy later\n",
    "        * For a normal (i.e., Gaussian) data distribution\n",
    "            * About 68% of values are within 1 standard deviation of mean\n",
    "            * About 95% of values are within 2 standard deviations\n",
    "            * About 99.7% of values are within 3 standard deviations"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
