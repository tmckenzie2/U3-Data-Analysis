{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 310](https://github.com/GonzagaCPSC310) Data Mining\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Linear Regression\n",
    "What are our learning objectives for this lesson?\n",
    "* Calculate a least squares linear regression line\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "In scatter plots, it can be nice to \"fit a line\"\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U3-Data-Analysis/master/figures/linear_regression_example.png\" width=\"600\"/>\n",
    "\n",
    "* this can be done via linear regression\n",
    "* we're going to look at a simple approach called \"Least Squares\"\n",
    "\n",
    "The basic idea: Given a set of points, find a line that \"best\" fits the points\n",
    "* i.e., find values for $m$ (slope) and $b$ (intercept) that best fits $y = mx + b$\n",
    "\n",
    "In least squares linear regression\n",
    "* find $m$ and $b$ that minimizes the sum of the (vertical) squared distance to the measured data points\n",
    "* once we find $m$, finding $b$ isn't difficult\n",
    "\n",
    "The basic least squares approach:\n",
    "1. Calculate the mean $\\bar{x}$ of the $x$ values and the mean $\\bar{y}$ of the $y$ values\n",
    "    * note the line must go through the point ($\\bar{x}$, $\\bar{y}$)\n",
    "2. Calculate the slope using the means (where n is the number of data points):\n",
    "$$m = \\frac{\\Sigma_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "3. Calculate the y intercept as b = ¯y − mx¯\n",
    "     * or, $\\bar{y} = m\\bar{x} + b$ ... since we know it must go through ($\\bar{x}$, $\\bar{y}$)\n",
    "     \n",
    "The correlation coefficient $r$ helps checks how good the linear relationship is:\n",
    "$$r = \\frac{\\Sigma_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma_{i=1}^{n}(x_i - \\bar{x})^2 \\Sigma_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "* note the bottom is essentially the same as the top just squared to strip away\n",
    "the signs\n",
    "* if the correlation is perfectly linear, then result is 1\n",
    "* if the correlation is perfect inverse linear, then result is -1\n",
    "* if no relationship, the result is 0\n",
    "\n",
    "An alternative formula (where $\\sigma_x$ is the standard deviation of $x$):\n",
    "$$m = r \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "The covariance can also be used to assess correlation\n",
    "$$cov = \\frac{\\Sigma_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n}$$\n",
    "* covariance can also be used to calculate the correlation coefficient:\n",
    "$$r = \\frac{cov}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "The standard error is also used to help check the fit\n",
    "$$stderr = \\sqrt{\\frac{\\Sigma_{i=1}^{n}(y_i - y^\\prime)^2}{n}}$$\n",
    "* where $y^\\prime$ is the \"predicted\" value and $y$ is the actual value\n",
    "* $(y_i - y^\\prime)$ is called a \"residual\"\n",
    "* note standard error is essentially the standard deviation of the differences\n",
    "* lower the value the \"better\" the fit\n",
    "\n",
    "Plus more along these lines (looking at the distribution of the \"residuals\")\n",
    "\n",
    "Some general hints for calculating values associated with linear regression\n",
    "* use `numpy.std(xs)` to calculate standard deviation\n",
    "* beware integer division (e.g., `sum(xs) // n`)\n",
    "* use list comprehensions, e.g.: \n",
    "```\n",
    "sum([(xs[i] - x avg)*(ys[i] - y avg) for i in range(n)])\n",
    "```\n",
    "\n",
    "Q: What does it mean if there is a strong (linear) correlation?\n",
    "* one of the attributes is (potentially) redundant because it is implied by the other\n",
    "* one is a good predictor for the other ... good if one is a class label\n",
    "* i.e., regression is one way to make predictions (more later)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
